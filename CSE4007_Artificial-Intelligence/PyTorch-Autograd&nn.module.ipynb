{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd2bf887",
   "metadata": {},
   "source": [
    "# Autograd\n",
    "- automatic differentiation engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8b1752",
   "metadata": {},
   "source": [
    "## backward()\n",
    "- Computes the sum of gradients of given tensors w.r.t. the leaves of  computation graphs. 주어진 텐서의 gradients 를 graph leaves 로 계산한다.\n",
    "\n",
    "=> gradient 계산 과정을 줄일 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a0edd3",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "- input\n",
    "    - $\\mathbf{w}, \\mathbf{x}$\n",
    "- model\n",
    "    - $a = \\mathbf{w}^T\\mathbf{x}$\n",
    "\n",
    "### 1. Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a23cac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ceab2d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 파라미터 생성, 입력값 생성\n",
    "w = torch.randn(2, requires_grad = True)\n",
    "x = torch.Tensor([1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe573973",
   "metadata": {},
   "source": [
    "### 2. Predict output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1fd5dd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model output 계산\n",
    "y_hat = torch.inner(w, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c09061",
   "metadata": {},
   "source": [
    "### 2-2. Intermediate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f3b585c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2.])\n",
      "tensor([-0.3628, -0.8129], requires_grad=True)\n",
      "tensor(-1.9887, grad_fn=<ReshapeAliasBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(w)\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0de6f20",
   "metadata": {},
   "source": [
    "### 3. Compute loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "27e59170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.1709, grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = (x.mean() - y_hat)**2\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd37f8d1",
   "metadata": {},
   "source": [
    "### 4. Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ee6b3b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff44470",
   "metadata": {},
   "source": [
    "### 4-2. Accessing the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2a38b055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -6.9774, -13.9547])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad\n",
    "# 업데이트 된 것을 확인 가능!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1249db",
   "metadata": {},
   "source": [
    "### Update parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7b47a31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "False\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1\n",
    "with torch.no_grad():\n",
    "    w = w - lr * w.grad\n",
    "    print(w.grad)\n",
    "print(w.requires_grad)\n",
    "w.requires_grad = True\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf34eb3",
   "metadata": {},
   "source": [
    "## Avoiding in-place operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8c990d2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n",
      "tensor([[[1., 2., 3.],\n",
      "         [4., 5., 6.],\n",
      "         [7., 8., 9.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "# 1. A = A + X\n",
    "# 2.\n",
    "mask = torch.ones_like(t)\n",
    "mask[1:, :] = 0\n",
    "print(mask)\n",
    "t = t*mask\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c026e8",
   "metadata": {},
   "source": [
    "# Implement a Shallow NN with PyTorch autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474ab084",
   "metadata": {},
   "source": [
    "## Data preparation & Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed742296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fa5221",
   "metadata": {},
   "source": [
    "### XOR data (numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e12cd52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_seeds = np.array([(0, 0), (1, 0), (0, 1), (1, 1)], dtype = np.float32)\n",
    "y_seeds = np.array([0, 1, 1, 0])\n",
    "\n",
    "N = 1000\n",
    "idxs = np.random.randint(0, 4, N)\n",
    "\n",
    "X = x_seeds[idxs]\n",
    "Y = y_seeds[idxs]\n",
    "\n",
    "X += np.random.normal(scale = 0.25, size = X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc91361b",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ddceda",
   "metadata": {},
   "source": [
    "### Model (torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a3486667",
   "metadata": {},
   "outputs": [],
   "source": [
    "class shallow_neural_network():\n",
    "    def __init__(self, num_input_features, num_hiddens):\n",
    "        self.num_input_features = num_input_features\n",
    "        self.num_hiddens = num_hiddens\n",
    "        \n",
    "        self.W1 = torch.randn((num_hiddens, num_input_features), requires_grad = True)\n",
    "        self.b1 = torch.randn(num_hiddens, requires_grad = True)\n",
    "        self.W2 = torch.randn(num_hiddens, requires_grad = True)\n",
    "        self.b2 = torch.randn(1, requires_grad = True)\n",
    "        \n",
    "        self.tanh = torch.nn.Tanh()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        z1 = torch.matmul(self.W1, x) + self.b1\n",
    "        a1 = self.tanh(z1)\n",
    "        z2 = torch.matmul(self.W2, a1) + self.b2\n",
    "        a2 = self.sigmoid(z2)\n",
    "        return a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6279d5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ag = shallow_neural_network(2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880248f8",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "baa57e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, model, lr = 0.1):\n",
    "    m = len(X)\n",
    "    \n",
    "    cost = 0.0\n",
    "    for x, y in zip(X, Y):\n",
    "        x_torch = torch.from_numpy(x) # tensor 로 바꿔줌.\n",
    "        \n",
    "        a2 = model.predict(x_torch)\n",
    "        if y == 1:\n",
    "            loss = -torch.log(a2+0.0001)\n",
    "        else:\n",
    "            loss = -torch.log(1.0001-a2)\n",
    "        \n",
    "        loss.backward() # compute gradients\n",
    "        cost += loss.item()\n",
    "        \n",
    "    with torch.no_grad(): # parameter update\n",
    "        model.W1 -= lr * model.W1.grad / m\n",
    "        model.b1 -= lr * model.b1.grad / m\n",
    "        model.W2 -= lr * model.W2.grad / m\n",
    "        model.b2 -= lr * model.b2.grad / m\n",
    "        \n",
    "    model.W1.requires_grad = True # parameter tracking\n",
    "    model.b1.requires_grad = True\n",
    "    model.W2.requires_grad = True\n",
    "    model.b2.requires_grad = True\n",
    "    \n",
    "    return cost/m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0aa51b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(100):\n",
    "    cost = train(X, Y, model_ag, 1.0)\n",
    "    if epoch % 10 == 0:\n",
    "        print(epoch, cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f104b3ec",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ef5137d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.1551e-17], grad_fn=<SigmoidBackward0>)\n",
      "tensor([1.], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.8453], grad_fn=<SigmoidBackward0>)\n",
      "tensor([8.6571e-18], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model_ag.predict(torch.Tensor((0, 0))))\n",
    "print(model_ag.predict(torch.Tensor((0, 1))))\n",
    "print(model_ag.predict(torch.Tensor((1, 0))))\n",
    "print(model_ag.predict(torch.Tensor((1, 1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626980e7",
   "metadata": {},
   "source": [
    "# nn.Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b882132d",
   "metadata": {},
   "source": [
    "## A simple custom module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d7699724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0721,  3.5980, -1.3363], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(in_features, out_features))\n",
    "        self.bias = nn.Parameter(torch.randn(out_features))\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return (input @ self.weight) + self.bias\n",
    "    \n",
    "m = MyLinear(4, 3)\n",
    "sample_input = torch.randn(4)\n",
    "m(sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcee7bb",
   "metadata": {},
   "source": [
    "## Modules as Building Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a872a131",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l0 = nn.Linear(4, 3)\n",
    "        self.l1 = nn.Linear(3, 1)\n",
    "    def forward(self, x):\n",
    "        x = self.l0(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.l1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1add2d13",
   "metadata": {},
   "source": [
    "1. Intialize\n",
    "\n",
    "`model = YourModel()`\n",
    "\n",
    "`optimizer = torch.optim.SGD(model.parameters(), lr = <learning_rate>`\n",
    "-> parameter 자동으로 셀렉. 알아서 optimize.\n",
    "\n",
    "2. Forward (= predict)\n",
    "\n",
    "`y_hat = model(input)`\n",
    "\n",
    "3. Backward (= update parameter)\n",
    "\n",
    "`loss = compute_loss(y, y_hat)`\n",
    "\n",
    "`model.zero_grad()` -> 누적 없앰.\n",
    "\n",
    "`loss.backward()`\n",
    "\n",
    "`optimizer.step()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e2f75e",
   "metadata": {},
   "source": [
    "# Implementing a Shallow NN with autograd and nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "99cb431e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "085b91d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "02c542b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_seeds = np.array([(0, 0), (1, 0), (0, 1), (1, 1)], dtype = np.float32)\n",
    "y_seeds = np.array([0, 1, 1, 0])\n",
    "\n",
    "N = 1000\n",
    "idxs = np.random.randint(0, 4, N)\n",
    "\n",
    "X = x_seeds[idxs]\n",
    "Y = y_seeds[idxs]\n",
    "\n",
    "X += np.random.normal(scale = 0.25, size = X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c018ef0",
   "metadata": {},
   "source": [
    "## Model (torch.nn.Module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e30c07d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class shallow_neural_network_nn(nn.Module):\n",
    "    def __init__(self, num_input_features, num_hiddens):\n",
    "        super().__init__()\n",
    "        self.num_input_features = num_input_features\n",
    "        self.num_hiddens = num_hiddens\n",
    "        \n",
    "        self.linear1 = nn.Linear(num_input_features, num_hiddens)\n",
    "        self.linear2 = nn.Linear(num_hiddens, 1)\n",
    "        \n",
    "        self.tanh = torch.nn.Tanh()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = self.linear1(x)\n",
    "        a1 = self.tanh(z1)\n",
    "        z2 = self.linear2(a1)\n",
    "        a2 = self.sigmoid(z2)\n",
    "        return a2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3732cdc",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3e514318",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "lr = 1.0\n",
    "num_hiddens = 3\n",
    "\n",
    "model_nn = shallow_neural_network_nn(2, num_hiddens)\n",
    "optimizer_nn = optim.SGD(model_nn.parameters(), lr = lr)\n",
    "loss_nn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "94e5226c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.6929, grad_fn=<DivBackward0>)\n",
      "10 tensor(0.6872, grad_fn=<DivBackward0>)\n",
      "20 tensor(0.6806, grad_fn=<DivBackward0>)\n",
      "30 tensor(0.6674, grad_fn=<DivBackward0>)\n",
      "40 tensor(0.6427, grad_fn=<DivBackward0>)\n",
      "50 tensor(0.6075, grad_fn=<DivBackward0>)\n",
      "60 tensor(0.5704, grad_fn=<DivBackward0>)\n",
      "70 tensor(0.5355, grad_fn=<DivBackward0>)\n",
      "80 tensor(0.4919, grad_fn=<DivBackward0>)\n",
      "90 tensor(0.4247, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    optimizer_nn.zero_grad()\n",
    "    \n",
    "    cost_nn = 0.0\n",
    "    for x, y in zip(X, Y):\n",
    "        x_torch = torch.FloatTensor(x)\n",
    "        y_torch = torch.FloatTensor([y])\n",
    "        \n",
    "        y_hat = model_nn(x_torch)\n",
    "        \n",
    "        loss_val = loss_nn(y_hat, y_torch)\n",
    "        cost_nn += loss_val\n",
    "        \n",
    "    cost_nn = cost_nn / len(X)\n",
    "    cost_nn.backward()\n",
    "    optimizer_nn.step()\n",
    "    \n",
    "    if epoch %10 == 0:\n",
    "        print(epoch, cost_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89758a4e",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e22905fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.]\n",
      "0 0.11121822893619537\n",
      "[1. 0.]\n",
      "1 0.7172693014144897\n",
      "[0. 1.]\n",
      "1 0.8491338491439819\n",
      "[1. 1.]\n",
      "0 0.3307785987854004\n"
     ]
    }
   ],
   "source": [
    "for x, y in zip(x_seeds, y_seeds):\n",
    "    print(x)\n",
    "    x_torch = torch.FloatTensor(x)\n",
    "    y_hat = model_nn(x_torch)\n",
    "    print(y, y_hat.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
